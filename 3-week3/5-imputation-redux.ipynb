{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Imputation Redux: Imputing Missing Categorical Data\n",
    "\n",
    "You might have realized that because OneHotEncoder fails with missing values, and SciKit learn prefers to work with numerical data, we face a bit of a conundrum when trying to deal with missing categorical values.  Note that we really don't want to use these imputers after a one-hot encoding either, because there is is no guarantee these imputers will follow the implicit rule that only one column of a one-hot encoded categorical set can be `1`. Here are two strategies.\n",
    "\n",
    "#### Using Pandas or Most Frequent Category\n",
    "\n",
    "You can use pandas to replace nulls, using one of the methods we covered previously. Alternatively, you can use SimpleImputer with the `strategy='most_frequent'` option to impute missing values with the most frequent category in each column before one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blue</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>red</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>red</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>green</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color  target\n",
       "0    red       1\n",
       "1   blue       0\n",
       "2    red       1\n",
       "3    red       0\n",
       "4    red       0\n",
       "5  green       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create DataFrame with missing values\n",
    "df = pd.DataFrame({\n",
    "    'color': ['red', 'blue', np.nan, 'red', np.nan, 'green'],\n",
    "    'target': [1, 0, 1, 0, 0, 1]\n",
    "})\n",
    "\n",
    "# Impute missing values\n",
    "imp = SimpleImputer(strategy='most_frequent')\n",
    "df['color'] = imp.fit_transform(df[['color']])[:,0]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a Different Library!\n",
    "\n",
    "As you might imagine, others have struggled with this, and so there are other libraries designed to address this problem.  For instance, the `fancyimpute` package has both a KNNImputer and an IterativeImputer you might try.  Here's an example with the `KNNImputer` from `fancyimpute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fancyimpute\n",
      "  Downloading fancyimpute-0.7.0.tar.gz (25 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting knnimpute>=0.1.0 (from fancyimpute)\n",
      "  Downloading knnimpute-0.1.0.tar.gz (8.3 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.2 in /home/codespace/.local/lib/python3.10/site-packages (from fancyimpute) (1.3.2)\n",
      "Collecting cvxpy (from fancyimpute)\n",
      "  Downloading cvxpy-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting cvxopt (from fancyimpute)\n",
      "  Downloading cvxopt-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting pytest (from fancyimpute)\n",
      "  Downloading pytest-8.0.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting nose (from fancyimpute)\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /home/codespace/.local/lib/python3.10/site-packages (from knnimpute>=0.1.0->fancyimpute) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.10 in /home/codespace/.local/lib/python3.10/site-packages (from knnimpute>=0.1.0->fancyimpute) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn>=0.24.2->fancyimpute) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn>=0.24.2->fancyimpute) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn>=0.24.2->fancyimpute) (3.2.0)\n",
      "Collecting osqp>=0.6.2 (from cvxpy->fancyimpute)\n",
      "  Downloading osqp-0.6.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting ecos>=2 (from cvxpy->fancyimpute)\n",
      "  Downloading ecos-2.0.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting clarabel>=0.5.0 (from cvxpy->fancyimpute)\n",
      "  Downloading clarabel-0.6.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting scs>=3.0 (from cvxpy->fancyimpute)\n",
      "  Downloading scs-3.2.4.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting pybind11 (from cvxpy->fancyimpute)\n",
      "  Downloading pybind11-2.11.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting iniconfig (from pytest->fancyimpute)\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from pytest->fancyimpute) (23.2)\n",
      "Collecting pluggy<2.0,>=1.3.0 (from pytest->fancyimpute)\n",
      "  Downloading pluggy-1.4.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/codespace/.local/lib/python3.10/site-packages (from pytest->fancyimpute) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from pytest->fancyimpute) (2.0.1)\n",
      "Collecting qdldl (from osqp>=0.6.2->cvxpy->fancyimpute)\n",
      "  Downloading qdldl-0.1.7.post0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Downloading cvxopt-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading cvxpy-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytest-8.0.0-py3-none-any.whl (334 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.0/334.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading clarabel-0.6.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading osqp-0.6.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (298 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pluggy-1.4.0-py3-none-any.whl (20 kB)\n",
      "Downloading scs-3.2.4.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading qdldl-0.1.7.post0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: fancyimpute, knnimpute\n",
      "  Building wheel for fancyimpute (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fancyimpute: filename=fancyimpute-0.7.0-py3-none-any.whl size=29880 sha256=b0b1ce384fad6c97878910390ca9f84e497657b764653944a4e4fe5c7ca61f94\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/7b/0c/d3/ee82d1fbdcc0858d96434af108608d01703505d453720c84ed\n",
      "  Building wheel for knnimpute (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for knnimpute: filename=knnimpute-0.1.0-py3-none-any.whl size=11329 sha256=155d396968ad6538be56a5587017b03c8b5cd144d50557ae2e53d05bae8f59c4\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/46/06/a5/45a724630562413c374e29c08732411d496092408b3a7bf754\n",
      "Successfully built fancyimpute knnimpute\n",
      "Installing collected packages: nose, pybind11, pluggy, knnimpute, iniconfig, cvxopt, scs, qdldl, pytest, ecos, clarabel, osqp, cvxpy, fancyimpute\n",
      "Successfully installed clarabel-0.6.0 cvxopt-1.3.2 cvxpy-1.4.2 ecos-2.0.12 fancyimpute-0.7.0 iniconfig-2.0.0 knnimpute-0.1.0 nose-1.3.7 osqp-0.6.3 pluggy-1.4.0 pybind11-2.11.1 pytest-8.0.0 qdldl-0.1.7.post0 scs-3.2.4.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install fancyimpute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### FancyImputes K-Nearest Neighbors (KNN) Imputer\n",
    "\n",
    "KNN from fancyimputer won't work with categorical data directly, but instead of using the `mean` (which is used by SciKit Learn's KNNImputer) is uses the `mode` for imputation, which is what we want.  To use KNN, first you should first encode your data using an `OrdinalEncoder` or `LabelEncoder`, then impute, then transform your data back into the categorical values you want.  This is more complicated than it should be because there is no easy way to preserve nulls in your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/6 with 0 missing, elapsed time: 0.000\n",
      "    Fruit   Color\n",
      "0   Apple     Red\n",
      "1  Banana  Yellow\n",
      "2  Cherry     Red\n",
      "3   Apple     Red\n",
      "4  Banana   Green\n",
      "5  Banana  Yellow\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fancyimpute import KNN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create DataFrame with missing values\n",
    "data = {\n",
    "    'Fruit': ['Apple', 'Banana', 'Cherry', 'Apple', None, 'Banana'],\n",
    "    'Color': ['Red', 'Yellow', 'Red', None, 'Green', 'Yellow']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Dictionary to hold LabelEncoders for each column\n",
    "encoders = {}\n",
    "\n",
    "# Replace categorical string values with numerical representations\n",
    "for col in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    not_null_mask = df[col].notnull()\n",
    "    df.loc[not_null_mask, col] = le.fit_transform(df.loc[not_null_mask, col].astype(str))\n",
    "    encoders[col] = le\n",
    "\n",
    "# Use KNN to impute the missing values\n",
    "knn_imputer = KNN()\n",
    "df_imputed = knn_imputer.fit_transform(df)\n",
    "\n",
    "# Round imputed values and convert to int for decoding\n",
    "# Note that the rounding is necessary because NaNs force columns to become floats\n",
    "df_imputed = pd.DataFrame(np.round(df_imputed), columns=df.columns).astype(int)\n",
    "\n",
    "# Decode imputed values back to original categorical values\n",
    "for col in df.columns:\n",
    "    df_imputed[col] = encoders[col].inverse_transform(df_imputed[col])\n",
    "\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other strategies may be applied in a similar manner, after which you can one-hot encode your data, and proceed with additional processing!\n",
    "\n",
    "Note that there is currently no elegant solution for imputation of categorical variables, and so if you want something more sophisticated than a SimpleImputer with a \"most_frequent\" strategy, you'll probably need to write some code.  However, we can turn the above method into our own \"Imputer\" class like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from fancyimpute import KNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CategoricalKNNImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, include_numeric=False, include_cols = []):\n",
    "        self.encoders = {}\n",
    "        self.knn_imputer = KNN()\n",
    "        self.include_numeric = include_numeric\n",
    "        self.include_cols = include_cols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        \n",
    "        if self.include_numeric:\n",
    "            self.cols = X.columns.tolist()\n",
    "        else:\n",
    "            self.cols = X.select_dtypes(include=['object', 'category']).columns.tolist()+self.include_cols\n",
    "            \n",
    "        for col in self.cols:\n",
    "            le = LabelEncoder()\n",
    "            not_null_mask = X[col].notnull()\n",
    "            if not_null_mask.sum() > 0:  # Only if there are non-null values to fit\n",
    "                X.loc[not_null_mask, col] = le.fit_transform(X.loc[not_null_mask, col].astype(str))\n",
    "                self.encoders[col] = le\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_original = X.copy()\n",
    "        X = X.copy()\n",
    "        \n",
    "        for col in self.cols:\n",
    "            if col in self.encoders:  # Only if encoder exists\n",
    "                not_null_mask = X[col].notnull()\n",
    "                X.loc[not_null_mask, col] = self.encoders[col].transform(X.loc[not_null_mask, col].astype(str))\n",
    "        \n",
    "        X_imputed = self.knn_imputer.fit_transform(X)\n",
    "        X_imputed = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "        \n",
    "        for col in self.cols:\n",
    "            if col in self.encoders:  # Only if encoder exists\n",
    "                X_imputed.loc[:, col] = np.round(X_imputed.loc[:, col])  # Rounding only categorical columns\n",
    "                X_imputed[col] = X_imputed[col].astype(int)  # Converting to int before decoding\n",
    "                X_imputed[col] = self.encoders[col].inverse_transform(X_imputed[col])\n",
    "        \n",
    "        if not self.include_numeric:\n",
    "            replacements = [x for x in X.columns if x not in self.cols]\n",
    "            #numeric_cols = X_original.select_dtypes(include=[np.number]).columns\n",
    "            X_imputed[replacements] = X_original[replacements]\n",
    "        \n",
    "        return X_imputed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The details of the Python might be more than you can understand at this point, but you should be able to recognize roughly what's going on here; we're simply building a component that works with SciKit Learn to do KNN based imputation on categorical columns.  You can apply this just like other SciKit Learn components, using `fit` and `transform`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Considerations\n",
    "\n",
    "When training machine learning models with imputed data, it's crucial to follow best practices to ensure the robustness and generalizability of your models. Here’s a guide that covers considerations like data leakage, when to use imputed data, and other relevant aspects:\n",
    "\n",
    "#### 1. Data Splitting\n",
    "Always split your dataset into training, validation (optional), and test sets before any imputation to avoid data leakage. Leakage occurs when information from the validation/test sets is used to inform any part of the modeling process, leading to overly optimistic performance estimates.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "#### 2. Imputation\n",
    "Perform imputation separately on each set:\n",
    "   - Fit the imputer on the training set.\n",
    "   - Transform both the training and test sets.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(X_train)  # Fit only on the training set\n",
    "\n",
    "X_train_imputed = imputer.transform(X_train)  # Transform the training set\n",
    "X_test_imputed = imputer.transform(X_test)  # Transform the test set\n",
    "```\n",
    "\n",
    "#### 3. When to Use Imputed Data\n",
    "- When the amount of missing data is substantial, imputation can leverage the available information, which would otherwise be discarded if only complete cases are used.\n",
    "- When the data are missing at random or missing completely at random, imputation can yield unbiased estimators.\n",
    "\n",
    "#### 4. When to Avoid Imputed Data\n",
    "- When missingness is related to the unobserved value itself (missing not at random), imputation might introduce bias.\n",
    "- When there are very few observed cases, imputation might overfit the training data, and it's better to use complete cases if available.\n",
    "\n",
    "#### 5. Model Evaluation\n",
    "- Evaluate model performance on the test set with imputed values, focusing on metrics relevant to your specific problem.\n",
    "- Consider performing sensitivity analyses by using different imputation methods and comparing the results.\n",
    "- Additionally, assess the model's performance on only complete cases in the test set, to understand how much information is gained (or lost) due to imputation.\n",
    "\n",
    "#### 6. Other Considerations\n",
    "- **Hyperparameter Tuning and Model Selection:** Conduct model selection and hyperparameter tuning using only the training set. Use techniques like cross-validation to assess model generalization on the training set before final evaluation on the test set.\n",
    "- **Complex Imputation Methods:** More advanced imputation methods like model-based imputation or multiple imputations may provide better results but come with their assumptions and computational cost.\n",
    "- **Documentation:** Document all the steps involved in the imputation process, the reasons for choosing a particular imputation method, and any assumptions made.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
